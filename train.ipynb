{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  85, 113, 117, 115, 118, 103, 112, 107, 109])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# poor man's data loader\n",
    "corpus_type = 'books'\n",
    "\n",
    "train_pt_file = f'data/{corpus_type}_train.pt'\n",
    "val_pt_file   = f'data/{corpus_type}_val.pt'\n",
    "meta_pt_file  = f'data/{corpus_type}_meta.pkl'\n",
    "model_pt_file = f'model/{corpus_type}.pt'\n",
    "model_finetuned_pt_file = f'model/{corpus_type}.pt'\n",
    "\n",
    "train_data = torch.load(train_pt_file)\n",
    "val_data   = torch.load(val_pt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 132 (inside data/meta.pkl)\n"
     ]
    }
   ],
   "source": [
    "# attempt to derive vocab_size from the dataset\n",
    "vocab_size = None\n",
    "if os.path.exists(meta_pt_file):\n",
    "    with open(meta_pt_file, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_pt_file})\")\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "10.766724 M parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 256 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "max_iters = 50_000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "# # chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "# # create a mapping from characters to integers\n",
    "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# # Train and test splits\n",
    "# data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_main_task_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "def get_batch(split):\n",
    "    return get_main_task_batch(split)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key   = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, hard_seq = False):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            if hard_seq == True:\n",
    "                idx_next = torch.argmax(probs, dim=1).reshape(1,-1) # (B, 1)\n",
    "            else:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "    \n",
    "model = BigramLanguageModel()\n",
    "print(device)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.0441, val loss 5.0465\n",
      "step 100: train loss 2.6633, val loss 2.6815\n",
      "step 200: train loss 2.2416, val loss 2.2667\n",
      "step 300: train loss 1.9490, val loss 1.9736\n",
      "step 400: train loss 1.7706, val loss 1.8002\n",
      "step 500: train loss 1.6615, val loss 1.6843\n",
      "step 600: train loss 1.5880, val loss 1.6135\n",
      "step 700: train loss 1.5351, val loss 1.5513\n",
      "step 800: train loss 1.4959, val loss 1.5165\n",
      "step 900: train loss 1.4683, val loss 1.4764\n",
      "step 1000: train loss 1.4352, val loss 1.4537\n",
      "step 1100: train loss 1.4139, val loss 1.4289\n",
      "step 1200: train loss 1.3980, val loss 1.4089\n",
      "step 1300: train loss 1.3781, val loss 1.3878\n",
      "step 1400: train loss 1.3661, val loss 1.3759\n",
      "step 1500: train loss 1.3490, val loss 1.3567\n",
      "step 1600: train loss 1.3400, val loss 1.3480\n",
      "step 1700: train loss 1.3276, val loss 1.3395\n",
      "step 1800: train loss 1.3119, val loss 1.3239\n",
      "step 1900: train loss 1.3095, val loss 1.3172\n",
      "step 2200: train loss 1.2838, val loss 1.2918\n",
      "step 2300: train loss 1.2796, val loss 1.2866\n",
      "step 2400: train loss 1.2687, val loss 1.2751\n",
      "step 2500: train loss 1.2618, val loss 1.2729\n",
      "step 2600: train loss 1.2650, val loss 1.2679\n",
      "step 2700: train loss 1.2559, val loss 1.2636\n",
      "step 2800: train loss 1.2464, val loss 1.2514\n",
      "step 2900: train loss 1.2444, val loss 1.2496\n",
      "step 3000: train loss 1.2399, val loss 1.2433\n",
      "step 3100: train loss 1.2353, val loss 1.2379\n",
      "step 3200: train loss 1.2309, val loss 1.2385\n",
      "step 3300: train loss 1.2303, val loss 1.2326\n",
      "step 3400: train loss 1.2203, val loss 1.2265\n",
      "step 3500: train loss 1.2199, val loss 1.2265\n",
      "step 3600: train loss 1.2172, val loss 1.2250\n",
      "step 3700: train loss 1.2127, val loss 1.2213\n",
      "step 3800: train loss 1.2097, val loss 1.2168\n",
      "step 3900: train loss 1.2057, val loss 1.2126\n",
      "step 4000: train loss 1.2032, val loss 1.2115\n",
      "step 4100: train loss 1.2015, val loss 1.2020\n",
      "step 4200: train loss 1.1982, val loss 1.2037\n",
      "step 4300: train loss 1.1971, val loss 1.2018\n",
      "step 4400: train loss 1.1929, val loss 1.1980\n",
      "step 4500: train loss 1.1903, val loss 1.1940\n",
      "step 4600: train loss 1.1886, val loss 1.2000\n",
      "step 4700: train loss 1.1895, val loss 1.1892\n",
      "step 4800: train loss 1.1818, val loss 1.1871\n",
      "step 5200: train loss 1.1716, val loss 1.1773\n",
      "step 5300: train loss 1.1721, val loss 1.1770\n",
      "step 5400: train loss 1.1691, val loss 1.1723\n",
      "step 5500: train loss 1.1681, val loss 1.1692\n",
      "step 5600: train loss 1.1677, val loss 1.1708\n",
      "step 5700: train loss 1.1602, val loss 1.1652\n",
      "step 5800: train loss 1.1604, val loss 1.1670\n",
      "step 5900: train loss 1.1599, val loss 1.1690\n",
      "step 6000: train loss 1.1557, val loss 1.1604\n",
      "step 6100: train loss 1.1578, val loss 1.1607\n",
      "step 6200: train loss 1.1540, val loss 1.1593\n",
      "step 6300: train loss 1.1523, val loss 1.1561\n",
      "step 6400: train loss 1.1515, val loss 1.1530\n",
      "step 6500: train loss 1.1490, val loss 1.1537\n",
      "step 6600: train loss 1.1502, val loss 1.1475\n",
      "step 6700: train loss 1.1465, val loss 1.1535\n",
      "step 6800: train loss 1.1464, val loss 1.1483\n",
      "step 6900: train loss 1.1451, val loss 1.1481\n",
      "step 7000: train loss 1.1423, val loss 1.1458\n",
      "step 7100: train loss 1.1406, val loss 1.1460\n",
      "step 7200: train loss 1.1397, val loss 1.1423\n",
      "step 7300: train loss 1.1408, val loss 1.1418\n",
      "step 7400: train loss 1.1396, val loss 1.1409\n",
      "step 7500: train loss 1.1406, val loss 1.1393\n",
      "step 7600: train loss 1.1344, val loss 1.1364\n",
      "step 7700: train loss 1.1342, val loss 1.1388\n",
      "step 7800: train loss 1.1327, val loss 1.1364\n",
      "step 7900: train loss 1.1326, val loss 1.1386\n",
      "step 8000: train loss 1.1292, val loss 1.1354\n",
      "step 8100: train loss 1.1311, val loss 1.1361\n",
      "step 8200: train loss 1.1246, val loss 1.1330\n",
      "step 8300: train loss 1.1279, val loss 1.1270\n",
      "step 8400: train loss 1.1240, val loss 1.1312\n",
      "step 8500: train loss 1.1255, val loss 1.1322\n",
      "step 8600: train loss 1.1252, val loss 1.1299\n",
      "step 8700: train loss 1.1220, val loss 1.1269\n",
      "step 8800: train loss 1.1196, val loss 1.1234\n",
      "step 8900: train loss 1.1201, val loss 1.1241\n",
      "step 9000: train loss 1.1208, val loss 1.1274\n",
      "step 9100: train loss 1.1189, val loss 1.1233\n",
      "step 9200: train loss 1.1162, val loss 1.1188\n",
      "step 9300: train loss 1.1160, val loss 1.1186\n",
      "step 9400: train loss 1.1114, val loss 1.1207\n",
      "step 9500: train loss 1.1108, val loss 1.1177\n",
      "step 9600: train loss 1.1174, val loss 1.1202\n",
      "step 9700: train loss 1.1115, val loss 1.1182\n",
      "step 9800: train loss 1.1100, val loss 1.1168\n",
      "step 9900: train loss 1.1113, val loss 1.1173\n",
      "step 10000: train loss 1.1075, val loss 1.1177\n",
      "step 10100: train loss 1.1073, val loss 1.1161\n",
      "step 10200: train loss 1.1081, val loss 1.1164\n",
      "step 10300: train loss 1.1036, val loss 1.1086\n",
      "step 10400: train loss 1.1072, val loss 1.1129\n",
      "step 10500: train loss 1.1050, val loss 1.1138\n",
      "step 10600: train loss 1.1057, val loss 1.1092\n",
      "step 10700: train loss 1.1020, val loss 1.1062\n",
      "step 10800: train loss 1.1043, val loss 1.1071\n",
      "step 10900: train loss 1.0995, val loss 1.1060\n",
      "step 11000: train loss 1.0985, val loss 1.1041\n",
      "step 11100: train loss 1.1019, val loss 1.1088\n",
      "step 11200: train loss 1.0982, val loss 1.1094\n",
      "step 11300: train loss 1.1018, val loss 1.1110\n",
      "step 11400: train loss 1.0977, val loss 1.1002\n",
      "step 11500: train loss 1.0970, val loss 1.1015\n",
      "step 11600: train loss 1.0951, val loss 1.1042\n",
      "step 11700: train loss 1.0937, val loss 1.1041\n",
      "step 11800: train loss 1.0961, val loss 1.1019\n",
      "step 11900: train loss 1.0961, val loss 1.1009\n",
      "step 12000: train loss 1.0925, val loss 1.0944\n",
      "step 12100: train loss 1.0907, val loss 1.1010\n",
      "step 12200: train loss 1.0925, val loss 1.0963\n",
      "step 12300: train loss 1.0940, val loss 1.1004\n",
      "step 12400: train loss 1.0922, val loss 1.0995\n",
      "step 12500: train loss 1.0887, val loss 1.0974\n",
      "step 12600: train loss 1.0903, val loss 1.0940\n",
      "step 12700: train loss 1.0894, val loss 1.0975\n",
      "step 12800: train loss 1.0878, val loss 1.0944\n",
      "step 12900: train loss 1.0889, val loss 1.0975\n",
      "step 13000: train loss 1.0859, val loss 1.0928\n",
      "step 13100: train loss 1.0849, val loss 1.0938\n",
      "step 13200: train loss 1.0846, val loss 1.0935\n",
      "step 13300: train loss 1.0866, val loss 1.0928\n",
      "step 13400: train loss 1.0851, val loss 1.0926\n",
      "step 13500: train loss 1.0814, val loss 1.0903\n",
      "step 13600: train loss 1.0840, val loss 1.0918\n",
      "step 13700: train loss 1.0860, val loss 1.0909\n",
      "step 13800: train loss 1.0805, val loss 1.0881\n",
      "step 13900: train loss 1.0817, val loss 1.0900\n",
      "step 14000: train loss 1.0812, val loss 1.0907\n",
      "step 14100: train loss 1.0831, val loss 1.0913\n",
      "step 14200: train loss 1.0840, val loss 1.0893\n",
      "step 14300: train loss 1.0788, val loss 1.0888\n",
      "step 14400: train loss 1.0765, val loss 1.0870\n",
      "step 14500: train loss 1.0796, val loss 1.0854\n",
      "step 14600: train loss 1.0754, val loss 1.0849\n",
      "step 14700: train loss 1.0798, val loss 1.0846\n",
      "step 14800: train loss 1.0741, val loss 1.0831\n",
      "step 14900: train loss 1.0735, val loss 1.0831\n",
      "step 15000: train loss 1.0740, val loss 1.0828\n",
      "step 15100: train loss 1.0761, val loss 1.0842\n",
      "step 15200: train loss 1.0745, val loss 1.0863\n",
      "step 15300: train loss 1.0737, val loss 1.0821\n",
      "step 15400: train loss 1.0736, val loss 1.0816\n",
      "step 15500: train loss 1.0722, val loss 1.0832\n",
      "step 15600: train loss 1.0726, val loss 1.0823\n",
      "step 15700: train loss 1.0733, val loss 1.0802\n",
      "step 15800: train loss 1.0718, val loss 1.0826\n",
      "step 15900: train loss 1.0725, val loss 1.0820\n",
      "step 16000: train loss 1.0684, val loss 1.0778\n",
      "step 16400: train loss 1.0658, val loss 1.0787\n",
      "step 16500: train loss 1.0668, val loss 1.0759\n",
      "step 16600: train loss 1.0688, val loss 1.0805\n",
      "step 16700: train loss 1.0689, val loss 1.0775\n",
      "step 16800: train loss 1.0666, val loss 1.0795\n",
      "step 16900: train loss 1.0681, val loss 1.0758\n",
      "step 17000: train loss 1.0672, val loss 1.0747\n",
      "step 17100: train loss 1.0682, val loss 1.0775\n",
      "step 17200: train loss 1.0636, val loss 1.0731\n",
      "step 17300: train loss 1.0641, val loss 1.0744\n",
      "step 17400: train loss 1.0630, val loss 1.0750\n",
      "step 17500: train loss 1.0658, val loss 1.0794\n",
      "step 17600: train loss 1.0615, val loss 1.0743\n",
      "step 17700: train loss 1.0613, val loss 1.0700\n",
      "step 17800: train loss 1.0635, val loss 1.0758\n",
      "step 17900: train loss 1.0617, val loss 1.0707\n",
      "step 18000: train loss 1.0624, val loss 1.0699\n",
      "step 18100: train loss 1.0598, val loss 1.0699\n",
      "step 18200: train loss 1.0593, val loss 1.0712\n",
      "step 18300: train loss 1.0611, val loss 1.0708\n",
      "step 18400: train loss 1.0597, val loss 1.0707\n",
      "step 18500: train loss 1.0607, val loss 1.0722\n",
      "step 18600: train loss 1.0595, val loss 1.0708\n",
      "step 18700: train loss 1.0578, val loss 1.0661\n",
      "step 18800: train loss 1.0587, val loss 1.0702\n",
      "step 18900: train loss 1.0569, val loss 1.0690\n",
      "step 19000: train loss 1.0540, val loss 1.0676\n",
      "step 19100: train loss 1.0585, val loss 1.0678\n",
      "step 19200: train loss 1.0551, val loss 1.0683\n",
      "step 19300: train loss 1.0582, val loss 1.0711\n",
      "step 19400: train loss 1.0550, val loss 1.0692\n",
      "step 19500: train loss 1.0549, val loss 1.0688\n",
      "step 19600: train loss 1.0551, val loss 1.0689\n",
      "step 19700: train loss 1.0533, val loss 1.0666\n",
      "step 19800: train loss 1.0538, val loss 1.0668\n",
      "step 19900: train loss 1.0547, val loss 1.0652\n",
      "step 20000: train loss 1.0550, val loss 1.0634\n",
      "step 20100: train loss 1.0550, val loss 1.0631\n",
      "step 20200: train loss 1.0543, val loss 1.0599\n",
      "step 20300: train loss 1.0546, val loss 1.0633\n",
      "step 20400: train loss 1.0546, val loss 1.0634\n",
      "step 20500: train loss 1.0513, val loss 1.0665\n",
      "step 20600: train loss 1.0531, val loss 1.0628\n",
      "step 20700: train loss 1.0518, val loss 1.0645\n",
      "step 20800: train loss 1.0475, val loss 1.0609\n",
      "step 20900: train loss 1.0508, val loss 1.0626\n",
      "step 21000: train loss 1.0483, val loss 1.0621\n",
      "step 21100: train loss 1.0516, val loss 1.0573\n",
      "step 21200: train loss 1.0524, val loss 1.0662\n",
      "step 21300: train loss 1.0526, val loss 1.0615\n",
      "step 21400: train loss 1.0514, val loss 1.0612\n",
      "step 21500: train loss 1.0517, val loss 1.0568\n",
      "step 21600: train loss 1.0520, val loss 1.0575\n",
      "step 21700: train loss 1.0467, val loss 1.0599\n",
      "step 21800: train loss 1.0453, val loss 1.0580\n",
      "step 21900: train loss 1.0483, val loss 1.0591\n",
      "step 22000: train loss 1.0521, val loss 1.0612\n",
      "step 22100: train loss 1.0469, val loss 1.0629\n",
      "step 22200: train loss 1.0482, val loss 1.0563\n",
      "step 22300: train loss 1.0497, val loss 1.0604\n",
      "step 22400: train loss 1.0448, val loss 1.0587\n",
      "step 22500: train loss 1.0443, val loss 1.0571\n",
      "step 22600: train loss 1.0479, val loss 1.0581\n",
      "step 22700: train loss 1.0473, val loss 1.0581\n",
      "step 23100: train loss 1.0440, val loss 1.0566\n",
      "step 23200: train loss 1.0421, val loss 1.0547\n",
      "step 23300: train loss 1.0451, val loss 1.0545\n",
      "step 23400: train loss 1.0412, val loss 1.0541\n",
      "step 23500: train loss 1.0433, val loss 1.0569\n",
      "step 23600: train loss 1.0432, val loss 1.0527\n",
      "step 23700: train loss 1.0435, val loss 1.0558\n",
      "step 23800: train loss 1.0431, val loss 1.0541\n",
      "step 23900: train loss 1.0418, val loss 1.0576\n",
      "step 24000: train loss 1.0427, val loss 1.0504\n",
      "step 24100: train loss 1.0403, val loss 1.0513\n",
      "step 24200: train loss 1.0446, val loss 1.0584\n",
      "step 24300: train loss 1.0413, val loss 1.0563\n",
      "step 24400: train loss 1.0427, val loss 1.0546\n",
      "step 24500: train loss 1.0437, val loss 1.0538\n",
      "step 24600: train loss 1.0396, val loss 1.0547\n",
      "step 24700: train loss 1.0422, val loss 1.0539\n",
      "step 24800: train loss 1.0400, val loss 1.0532\n",
      "step 24900: train loss 1.0380, val loss 1.0515\n",
      "step 25000: train loss 1.0410, val loss 1.0543\n",
      "step 25100: train loss 1.0377, val loss 1.0509\n",
      "step 25200: train loss 1.0389, val loss 1.0523\n",
      "step 25300: train loss 1.0392, val loss 1.0514\n",
      "step 25400: train loss 1.0368, val loss 1.0481\n",
      "step 25500: train loss 1.0374, val loss 1.0502\n",
      "step 25600: train loss 1.0384, val loss 1.0504\n",
      "step 25700: train loss 1.0376, val loss 1.0552\n",
      "step 25800: train loss 1.0376, val loss 1.0502\n",
      "step 25900: train loss 1.0335, val loss 1.0461\n",
      "step 26000: train loss 1.0318, val loss 1.0480\n",
      "step 26100: train loss 1.0370, val loss 1.0475\n",
      "step 26200: train loss 1.0350, val loss 1.0482\n",
      "step 26300: train loss 1.0359, val loss 1.0506\n",
      "step 26400: train loss 1.0347, val loss 1.0488\n",
      "step 26500: train loss 1.0357, val loss 1.0468\n",
      "step 26600: train loss 1.0364, val loss 1.0526\n",
      "step 26700: train loss 1.0352, val loss 1.0466\n",
      "step 26800: train loss 1.0339, val loss 1.0490\n",
      "step 26900: train loss 1.0359, val loss 1.0490\n",
      "step 27000: train loss 1.0364, val loss 1.0490\n",
      "step 27100: train loss 1.0369, val loss 1.0497\n",
      "step 27200: train loss 1.0358, val loss 1.0446\n",
      "step 27300: train loss 1.0363, val loss 1.0480\n",
      "step 27400: train loss 1.0328, val loss 1.0455\n",
      "step 27500: train loss 1.0305, val loss 1.0437\n",
      "step 27600: train loss 1.0346, val loss 1.0459\n",
      "step 27700: train loss 1.0337, val loss 1.0477\n",
      "step 27800: train loss 1.0320, val loss 1.0484\n",
      "step 27900: train loss 1.0314, val loss 1.0387\n",
      "step 28000: train loss 1.0335, val loss 1.0461\n",
      "step 28100: train loss 1.0334, val loss 1.0447\n",
      "step 28200: train loss 1.0306, val loss 1.0436\n",
      "step 28300: train loss 1.0304, val loss 1.0445\n",
      "step 28400: train loss 1.0315, val loss 1.0447\n",
      "step 28500: train loss 1.0304, val loss 1.0448\n",
      "step 28600: train loss 1.0296, val loss 1.0411\n",
      "step 28700: train loss 1.0300, val loss 1.0412\n",
      "step 28800: train loss 1.0310, val loss 1.0474\n",
      "step 28900: train loss 1.0278, val loss 1.0432\n",
      "step 29000: train loss 1.0261, val loss 1.0402\n",
      "step 29100: train loss 1.0308, val loss 1.0431\n",
      "step 29200: train loss 1.0296, val loss 1.0418\n",
      "step 29300: train loss 1.0292, val loss 1.0434\n",
      "step 29400: train loss 1.0298, val loss 1.0392\n",
      "step 29500: train loss 1.0249, val loss 1.0430\n",
      "step 29600: train loss 1.0263, val loss 1.0392\n",
      "step 29700: train loss 1.0249, val loss 1.0431\n",
      "step 29800: train loss 1.0288, val loss 1.0411\n",
      "step 29900: train loss 1.0260, val loss 1.0424\n",
      "step 30000: train loss 1.0290, val loss 1.0441\n",
      "step 30100: train loss 1.0230, val loss 1.0426\n",
      "step 30200: train loss 1.0293, val loss 1.0405\n",
      "step 30300: train loss 1.0250, val loss 1.0383\n",
      "step 30400: train loss 1.0239, val loss 1.0394\n",
      "step 30500: train loss 1.0286, val loss 1.0432\n",
      "step 30600: train loss 1.0230, val loss 1.0405\n",
      "step 30700: train loss 1.0265, val loss 1.0433\n",
      "step 30800: train loss 1.0234, val loss 1.0396\n",
      "step 30900: train loss 1.0239, val loss 1.0355\n",
      "step 31000: train loss 1.0254, val loss 1.0389\n",
      "step 31100: train loss 1.0245, val loss 1.0355\n",
      "step 31200: train loss 1.0249, val loss 1.0389\n",
      "step 31300: train loss 1.0252, val loss 1.0412\n",
      "step 31400: train loss 1.0229, val loss 1.0377\n",
      "step 31500: train loss 1.0212, val loss 1.0400\n",
      "step 31600: train loss 1.0207, val loss 1.0369\n",
      "step 31700: train loss 1.0266, val loss 1.0387\n",
      "step 31800: train loss 1.0227, val loss 1.0415\n",
      "step 31900: train loss 1.0227, val loss 1.0388\n",
      "step 32000: train loss 1.0233, val loss 1.0374\n",
      "step 32100: train loss 1.0220, val loss 1.0323\n",
      "step 32200: train loss 1.0250, val loss 1.0432\n",
      "step 32300: train loss 1.0234, val loss 1.0385\n",
      "step 32400: train loss 1.0224, val loss 1.0393\n",
      "step 32500: train loss 1.0228, val loss 1.0370\n",
      "step 32600: train loss 1.0226, val loss 1.0392\n",
      "step 32700: train loss 1.0210, val loss 1.0348\n",
      "step 32800: train loss 1.0217, val loss 1.0390\n",
      "step 32900: train loss 1.0195, val loss 1.0335\n",
      "step 33000: train loss 1.0252, val loss 1.0375\n",
      "step 33100: train loss 1.0197, val loss 1.0340\n",
      "step 33200: train loss 1.0209, val loss 1.0371\n",
      "step 33300: train loss 1.0208, val loss 1.0367\n",
      "step 33400: train loss 1.0179, val loss 1.0351\n",
      "step 33500: train loss 1.0226, val loss 1.0421\n",
      "step 33600: train loss 1.0193, val loss 1.0392\n",
      "step 33700: train loss 1.0186, val loss 1.0334\n",
      "step 33800: train loss 1.0194, val loss 1.0350\n",
      "step 33900: train loss 1.0216, val loss 1.0336\n",
      "step 34000: train loss 1.0188, val loss 1.0366\n",
      "step 34100: train loss 1.0203, val loss 1.0365\n",
      "step 34200: train loss 1.0226, val loss 1.0390\n",
      "step 34300: train loss 1.0169, val loss 1.0343\n",
      "step 34400: train loss 1.0162, val loss 1.0326\n",
      "step 34500: train loss 1.0186, val loss 1.0338\n",
      "step 34600: train loss 1.0204, val loss 1.0315\n",
      "step 34700: train loss 1.0189, val loss 1.0376\n",
      "step 34800: train loss 1.0194, val loss 1.0354\n",
      "step 34900: train loss 1.0157, val loss 1.0336\n",
      "step 35000: train loss 1.0202, val loss 1.0340\n",
      "step 35100: train loss 1.0184, val loss 1.0321\n",
      "step 35200: train loss 1.0154, val loss 1.0332\n",
      "step 35300: train loss 1.0163, val loss 1.0291\n",
      "step 35400: train loss 1.0164, val loss 1.0308\n",
      "step 35500: train loss 1.0191, val loss 1.0307\n",
      "step 35600: train loss 1.0170, val loss 1.0373\n",
      "step 35700: train loss 1.0139, val loss 1.0352\n",
      "step 35800: train loss 1.0187, val loss 1.0332\n",
      "step 35900: train loss 1.0183, val loss 1.0373\n",
      "step 36000: train loss 1.0137, val loss 1.0282\n",
      "step 36100: train loss 1.0119, val loss 1.0312\n",
      "step 36200: train loss 1.0139, val loss 1.0327\n",
      "step 36300: train loss 1.0157, val loss 1.0302\n",
      "step 36400: train loss 1.0136, val loss 1.0313\n",
      "step 36500: train loss 1.0152, val loss 1.0302\n",
      "step 36600: train loss 1.0147, val loss 1.0316\n",
      "step 36700: train loss 1.0138, val loss 1.0349\n",
      "step 36800: train loss 1.0174, val loss 1.0310\n",
      "step 36900: train loss 1.0173, val loss 1.0321\n",
      "step 37000: train loss 1.0132, val loss 1.0337\n",
      "step 37100: train loss 1.0150, val loss 1.0288\n",
      "step 37200: train loss 1.0149, val loss 1.0312\n",
      "step 37300: train loss 1.0149, val loss 1.0328\n",
      "step 37400: train loss 1.0112, val loss 1.0347\n",
      "step 37500: train loss 1.0109, val loss 1.0307\n",
      "step 37600: train loss 1.0142, val loss 1.0357\n",
      "step 37700: train loss 1.0137, val loss 1.0279\n",
      "step 37800: train loss 1.0120, val loss 1.0294\n",
      "step 37900: train loss 1.0135, val loss 1.0308\n",
      "step 38000: train loss 1.0141, val loss 1.0321\n",
      "step 38100: train loss 1.0102, val loss 1.0321\n",
      "step 38200: train loss 1.0090, val loss 1.0275\n",
      "step 38300: train loss 1.0106, val loss 1.0280\n",
      "step 38400: train loss 1.0110, val loss 1.0292\n",
      "step 38500: train loss 1.0126, val loss 1.0301\n",
      "step 38600: train loss 1.0139, val loss 1.0305\n",
      "step 38700: train loss 1.0087, val loss 1.0292\n",
      "step 38800: train loss 1.0100, val loss 1.0289\n",
      "step 38900: train loss 1.0139, val loss 1.0284\n",
      "step 39000: train loss 1.0125, val loss 1.0314\n",
      "step 39100: train loss 1.0129, val loss 1.0303\n",
      "step 39200: train loss 1.0069, val loss 1.0286\n",
      "step 39300: train loss 1.0132, val loss 1.0329\n",
      "step 39500: train loss 1.0109, val loss 1.0277\n",
      "step 39600: train loss 1.0118, val loss 1.0288\n",
      "step 39700: train loss 1.0071, val loss 1.0282\n",
      "step 39800: train loss 1.0129, val loss 1.0325\n",
      "step 39900: train loss 1.0114, val loss 1.0300\n",
      "step 40000: train loss 1.0099, val loss 1.0301\n",
      "step 40100: train loss 1.0089, val loss 1.0316\n",
      "step 40200: train loss 1.0103, val loss 1.0273\n",
      "step 40300: train loss 1.0100, val loss 1.0272\n",
      "step 40400: train loss 1.0099, val loss 1.0262\n",
      "step 40500: train loss 1.0110, val loss 1.0316\n",
      "step 40600: train loss 1.0088, val loss 1.0252\n",
      "step 40700: train loss 1.0055, val loss 1.0250\n",
      "step 40800: train loss 1.0085, val loss 1.0264\n",
      "step 40900: train loss 1.0097, val loss 1.0264\n",
      "step 41000: train loss 1.0081, val loss 1.0243\n",
      "step 41100: train loss 1.0085, val loss 1.0271\n",
      "step 41200: train loss 1.0094, val loss 1.0238\n",
      "step 41300: train loss 1.0111, val loss 1.0304\n",
      "step 41400: train loss 1.0116, val loss 1.0283\n",
      "step 41500: train loss 1.0064, val loss 1.0244\n",
      "step 41600: train loss 1.0082, val loss 1.0251\n",
      "step 41700: train loss 1.0063, val loss 1.0228\n",
      "step 41800: train loss 1.0100, val loss 1.0230\n",
      "step 41900: train loss 1.0076, val loss 1.0290\n",
      "step 42000: train loss 1.0065, val loss 1.0217\n",
      "step 42100: train loss 1.0099, val loss 1.0247\n",
      "step 42200: train loss 1.0037, val loss 1.0226\n",
      "step 42300: train loss 1.0052, val loss 1.0247\n",
      "step 42400: train loss 1.0096, val loss 1.0274\n",
      "step 42500: train loss 1.0065, val loss 1.0262\n",
      "step 42600: train loss 1.0028, val loss 1.0242\n",
      "step 42700: train loss 1.0053, val loss 1.0224\n",
      "step 42800: train loss 1.0066, val loss 1.0290\n",
      "step 42900: train loss 1.0057, val loss 1.0249\n",
      "step 43000: train loss 1.0052, val loss 1.0235\n",
      "step 43100: train loss 1.0051, val loss 1.0223\n",
      "step 43200: train loss 1.0069, val loss 1.0259\n",
      "step 43300: train loss 1.0064, val loss 1.0207\n",
      "step 43400: train loss 1.0063, val loss 1.0218\n",
      "step 43500: train loss 1.0058, val loss 1.0246\n",
      "step 43600: train loss 1.0049, val loss 1.0281\n",
      "step 43700: train loss 1.0067, val loss 1.0221\n",
      "step 43800: train loss 1.0071, val loss 1.0248\n",
      "step 43900: train loss 1.0057, val loss 1.0248\n",
      "step 44000: train loss 1.0048, val loss 1.0238\n",
      "step 44100: train loss 1.0070, val loss 1.0244\n",
      "step 44200: train loss 1.0072, val loss 1.0204\n",
      "step 44300: train loss 1.0004, val loss 1.0188\n",
      "step 44400: train loss 1.0024, val loss 1.0232\n",
      "step 44500: train loss 1.0056, val loss 1.0230\n",
      "step 44600: train loss 1.0032, val loss 1.0191\n",
      "step 44700: train loss 1.0030, val loss 1.0209\n",
      "step 44800: train loss 1.0039, val loss 1.0231\n",
      "step 44900: train loss 1.0031, val loss 1.0212\n",
      "step 45000: train loss 0.9995, val loss 1.0232\n",
      "step 45100: train loss 1.0062, val loss 1.0224\n",
      "step 45200: train loss 1.0038, val loss 1.0207\n",
      "step 45300: train loss 1.0040, val loss 1.0216\n",
      "step 45400: train loss 1.0040, val loss 1.0224\n",
      "step 45500: train loss 1.0029, val loss 1.0229\n",
      "step 45600: train loss 1.0029, val loss 1.0219\n",
      "step 45700: train loss 1.0025, val loss 1.0252\n",
      "step 45800: train loss 1.0047, val loss 1.0228\n",
      "step 45900: train loss 1.0002, val loss 1.0213\n",
      "step 46000: train loss 1.0045, val loss 1.0246\n",
      "step 46100: train loss 1.0018, val loss 1.0221\n",
      "step 46200: train loss 0.9999, val loss 1.0190\n",
      "step 46300: train loss 1.0035, val loss 1.0233\n",
      "step 46400: train loss 1.0028, val loss 1.0180\n",
      "step 46500: train loss 1.0007, val loss 1.0192\n",
      "step 46600: train loss 0.9984, val loss 1.0203\n",
      "step 46700: train loss 0.9985, val loss 1.0219\n",
      "step 46800: train loss 0.9999, val loss 1.0198\n",
      "step 46900: train loss 1.0013, val loss 1.0213\n",
      "step 47000: train loss 1.0006, val loss 1.0234\n",
      "step 47100: train loss 1.0000, val loss 1.0171\n",
      "step 47200: train loss 1.0031, val loss 1.0233\n",
      "step 47300: train loss 1.0008, val loss 1.0201\n",
      "step 47400: train loss 1.0006, val loss 1.0205\n",
      "step 47500: train loss 0.9980, val loss 1.0214\n",
      "step 47600: train loss 1.0004, val loss 1.0207\n",
      "step 47700: train loss 0.9989, val loss 1.0230\n",
      "step 47800: train loss 0.9996, val loss 1.0218\n",
      "step 47900: train loss 1.0005, val loss 1.0190\n",
      "step 48000: train loss 0.9995, val loss 1.0171\n",
      "step 48100: train loss 1.0034, val loss 1.0203\n",
      "step 48200: train loss 1.0005, val loss 1.0188\n",
      "step 48300: train loss 1.0004, val loss 1.0214\n",
      "step 48400: train loss 0.9993, val loss 1.0145\n",
      "step 48500: train loss 0.9981, val loss 1.0180\n",
      "step 48600: train loss 0.9978, val loss 1.0196\n",
      "step 48700: train loss 1.0003, val loss 1.0216\n",
      "step 48800: train loss 0.9979, val loss 1.0197\n",
      "step 48900: train loss 0.9953, val loss 1.0177\n",
      "step 49000: train loss 1.0010, val loss 1.0186\n",
      "step 49100: train loss 0.9990, val loss 1.0191\n",
      "step 49200: train loss 0.9972, val loss 1.0166\n",
      "step 49300: train loss 0.9965, val loss 1.0177\n",
      "step 49400: train loss 0.9985, val loss 1.0207\n",
      "step 49500: train loss 0.9979, val loss 1.0170\n",
      "step 49600: train loss 0.9990, val loss 1.0183\n",
      "step 49700: train loss 0.9986, val loss 1.0182\n",
      "step 49800: train loss 0.9975, val loss 1.0177\n",
      "step 49900: train loss 0.9998, val loss 1.0165\n",
      "step 49999: train loss 0.9984, val loss 1.0184\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model'):\n",
    "    !mkdir model/\n",
    "    torch.save(m.state_dict(), model_pt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "10.766724 M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finetune: see Finetuning is no different than training, we just make sure to initialize from a pretrained model and train with a smaller learning rate\n",
    "max_iters = 20_000\n",
    "# finetune at constant LR\n",
    "learning_rate = 3e-5\n",
    "decay_lr = False\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "print(device)\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "m.load_state_dict(torch.load(model_pt_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = ' '\n",
    "STOP = '.'\n",
    "REVERSE_TOK = torch.tensor(encode('ПФ='), dtype=torch.long)\n",
    "PAREM_TOK = torch.tensor(encode('ПП='), dtype=torch.long)\n",
    "PAD_TOK = torch.tensor(encode(PAD), dtype=torch.long)\n",
    "STOP_TOK = torch.tensor(encode(STOP), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "parems_list = []\n",
    "with open('data/parems_cleaned.txt') as f:\n",
    "    for line in f:\n",
    "        parems_list.append(line.strip('\\r\\n'))\n",
    "        \n",
    "n = int(0.9*len(parems_list)) # first 90% will be train, rest val\n",
    "indicies = [i for i in range(len(parems_list))]\n",
    "np.random.shuffle(parems_list)\n",
    "# parems_list = parems_list[]\n",
    "parems_train_data = parems_list[:n]\n",
    "parems_val_data   = parems_list[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parems_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = parems_train_data if split == 'train' else parems_val_data\n",
    "    # take random parem\n",
    "    ix = torch.randint(0, len(data), (1, ))\n",
    "    sent = torch.tensor(encode(' '.join([data[i] for i in ix])), dtype=torch.long)\n",
    "    # divide it randomly - may not from 1????\n",
    "    j = torch.randint(1, len(sent), (1,))\n",
    "#     print(sent)\n",
    "    t = torch.cat([\n",
    "        sent[:j],\n",
    "        PAREM_TOK, \n",
    "        sent[j:],\n",
    "        STOP_TOK,\n",
    "        PAD_TOK.repeat(max(0,2*block_size - len(sent) - 4))\n",
    "    ], dim=0) \n",
    "#     print(t.shape)\n",
    "    bs = min(len(sent)+j, block_size)\n",
    "    x = torch.stack([t[i:i+block_size] for i in range(bs)])\n",
    "    y = torch.stack([t[i+1:i+1+block_size] for i in range(bs)])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reverse_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    i = torch.randint(len(data) - block_size, (1, ))\n",
    "    j = torch.randint(block_size//2,  block_size, (1,))\n",
    "    sent = data[i:i+j]\n",
    "    sent_len = len(sent)\n",
    "    t = torch.cat([\n",
    "        sent, \n",
    "        REVERSE_TOK, \n",
    "#         SEP_TOK, \n",
    "        torch.flip(sent, dims=(0,)),\n",
    "        STOP_TOK,\n",
    "        PAD_TOK.repeat(max(0,2*block_size - 2*sent_len - 4))\n",
    "    ], dim=0) \n",
    "    # print(input_tensor)\n",
    "    # print(output_tensor)\n",
    "#     print(t)\n",
    "    x = torch.stack([t[i:i+block_size] for i in range(block_size)])\n",
    "    y = torch.stack([t[i+1:i+1+block_size] for i in range(block_size)])\n",
    "    # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    task0_x, task0_y = get_main_task_batch(split) # не забываем про основную задачу\n",
    "    task1_x, task1_y = get_reverse_batch(split)   # учим на переворот\n",
    "    task2_x, task2_y = get_parems_batch(split)    # учим на пословицы\n",
    "    return torch.cat([task0_x, task1_x, task2_x], dim=0), torch.cat([task0_y, task1_y, task2_y], dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 1.5985, val loss 1.6149\n",
      "step 100: train loss 1.2622, val loss 1.2727\n",
      "step 200: train loss 1.2079, val loss 1.2300\n",
      "step 300: train loss 1.1858, val loss 1.1985\n",
      "step 400: train loss 1.1841, val loss 1.2007\n",
      "step 500: train loss 1.1692, val loss 1.1918\n",
      "step 600: train loss 1.1620, val loss 1.1858\n",
      "step 700: train loss 1.1649, val loss 1.1786\n",
      "step 800: train loss 1.1562, val loss 1.1698\n",
      "step 900: train loss 1.1520, val loss 1.1700\n",
      "step 1000: train loss 1.1512, val loss 1.1551\n",
      "step 1100: train loss 1.1424, val loss 1.1529\n",
      "step 1200: train loss 1.1397, val loss 1.1556\n",
      "step 1300: train loss 1.1347, val loss 1.1488\n",
      "step 1400: train loss 1.1221, val loss 1.1497\n",
      "step 1500: train loss 1.1279, val loss 1.1392\n",
      "step 1600: train loss 1.1250, val loss 1.1447\n",
      "step 1700: train loss 1.1224, val loss 1.1381\n",
      "step 1800: train loss 1.1184, val loss 1.1298\n",
      "step 1900: train loss 1.1199, val loss 1.1280\n",
      "step 2000: train loss 1.1193, val loss 1.1268\n",
      "step 2100: train loss 1.1186, val loss 1.1243\n",
      "step 2200: train loss 1.1129, val loss 1.1304\n",
      "step 2300: train loss 1.1119, val loss 1.1230\n",
      "step 2400: train loss 1.1080, val loss 1.1250\n",
      "step 2500: train loss 1.1017, val loss 1.1210\n",
      "step 2600: train loss 1.0994, val loss 1.1247\n",
      "step 2700: train loss 1.1011, val loss 1.1107\n",
      "step 2800: train loss 1.0996, val loss 1.1115\n",
      "step 2900: train loss 1.0958, val loss 1.1230\n",
      "step 3000: train loss 1.0912, val loss 1.1116\n",
      "step 3100: train loss 1.0914, val loss 1.1179\n",
      "step 3200: train loss 1.0906, val loss 1.1156\n",
      "step 3300: train loss 1.0859, val loss 1.1121\n",
      "step 3400: train loss 1.0888, val loss 1.1057\n",
      "step 3500: train loss 1.0862, val loss 1.1007\n",
      "step 3600: train loss 1.0782, val loss 1.1029\n",
      "step 3700: train loss 1.0775, val loss 1.1070\n",
      "step 3800: train loss 1.0785, val loss 1.0993\n",
      "step 3900: train loss 1.0848, val loss 1.1100\n",
      "step 4000: train loss 1.0835, val loss 1.0996\n",
      "step 4100: train loss 1.0856, val loss 1.0951\n",
      "step 4200: train loss 1.0794, val loss 1.0999\n",
      "step 4300: train loss 1.0807, val loss 1.0958\n",
      "step 4400: train loss 1.0795, val loss 1.0926\n",
      "step 4500: train loss 1.0762, val loss 1.0929\n",
      "step 4600: train loss 1.0700, val loss 1.0974\n",
      "step 4700: train loss 1.0720, val loss 1.0956\n",
      "step 4800: train loss 1.0700, val loss 1.0979\n",
      "step 5100: train loss 1.0670, val loss 1.0898\n",
      "step 5200: train loss 1.0738, val loss 1.0902\n",
      "step 5300: train loss 1.0780, val loss 1.0863\n",
      "step 5400: train loss 1.0692, val loss 1.0867\n",
      "step 5500: train loss 1.0687, val loss 1.0865\n",
      "step 5600: train loss 1.0698, val loss 1.0847\n",
      "step 5700: train loss 1.0673, val loss 1.0903\n",
      "step 5800: train loss 1.0648, val loss 1.0858\n",
      "step 5900: train loss 1.0680, val loss 1.0852\n",
      "step 6000: train loss 1.0631, val loss 1.0907\n",
      "step 6100: train loss 1.0592, val loss 1.0776\n",
      "step 6200: train loss 1.0614, val loss 1.0827\n",
      "step 6300: train loss 1.0662, val loss 1.0841\n",
      "step 6400: train loss 1.0605, val loss 1.0753\n",
      "step 6500: train loss 1.0588, val loss 1.0843\n",
      "step 6600: train loss 1.0623, val loss 1.0788\n",
      "step 6700: train loss 1.0668, val loss 1.0831\n",
      "step 6800: train loss 1.0645, val loss 1.0750\n",
      "step 6900: train loss 1.0583, val loss 1.0774\n",
      "step 7000: train loss 1.0630, val loss 1.0785\n",
      "step 7100: train loss 1.0544, val loss 1.0737\n",
      "step 7200: train loss 1.0538, val loss 1.0842\n",
      "step 7300: train loss 1.0540, val loss 1.0746\n",
      "step 7400: train loss 1.0558, val loss 1.0801\n",
      "step 7500: train loss 1.0576, val loss 1.0737\n",
      "step 7600: train loss 1.0581, val loss 1.0703\n",
      "step 7700: train loss 1.0590, val loss 1.0719\n",
      "step 7800: train loss 1.0522, val loss 1.0701\n",
      "step 7900: train loss 1.0421, val loss 1.0675\n",
      "step 8000: train loss 1.0566, val loss 1.0735\n",
      "step 8100: train loss 1.0530, val loss 1.0721\n",
      "step 8200: train loss 1.0481, val loss 1.0700\n",
      "step 8300: train loss 1.0506, val loss 1.0708\n",
      "step 8400: train loss 1.0548, val loss 1.0783\n",
      "step 8500: train loss 1.0519, val loss 1.0645\n",
      "step 8600: train loss 1.0492, val loss 1.0648\n",
      "step 8700: train loss 1.0515, val loss 1.0676\n",
      "step 8800: train loss 1.0540, val loss 1.0669\n",
      "step 8900: train loss 1.0444, val loss 1.0648\n",
      "step 9000: train loss 1.0375, val loss 1.0689\n",
      "step 9100: train loss 1.0477, val loss 1.0665\n",
      "step 9200: train loss 1.0552, val loss 1.0656\n",
      "step 9300: train loss 1.0387, val loss 1.0651\n",
      "step 9400: train loss 1.0369, val loss 1.0591\n",
      "step 9500: train loss 1.0502, val loss 1.0691\n",
      "step 9600: train loss 1.0468, val loss 1.0636\n",
      "step 9700: train loss 1.0383, val loss 1.0627\n",
      "step 9800: train loss 1.0445, val loss 1.0575\n",
      "step 9900: train loss 1.0379, val loss 1.0616\n",
      "step 10000: train loss 1.0398, val loss 1.0617\n",
      "step 10100: train loss 1.0426, val loss 1.0581\n",
      "step 10200: train loss 1.0374, val loss 1.0610\n",
      "step 10300: train loss 1.0458, val loss 1.0583\n",
      "step 10400: train loss 1.0408, val loss 1.0613\n",
      "step 10500: train loss 1.0443, val loss 1.0604\n",
      "step 10600: train loss 1.0326, val loss 1.0614\n",
      "step 10700: train loss 1.0400, val loss 1.0590\n",
      "step 10800: train loss 1.0403, val loss 1.0579\n",
      "step 10900: train loss 1.0392, val loss 1.0568\n",
      "step 11000: train loss 1.0378, val loss 1.0610\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = parems_list[12][:30] #decode(data[0:0+32].tolist())\n",
    "context = torch.cat([\n",
    "#     REVERSE_TOK, \n",
    "    torch.tensor(encode(sent), dtype=torch.long),\n",
    "    REVERSE_TOK, \n",
    "    ], dim=0).reshape(1,-1).to(device)# torch.zeroes((1,1), dtype=torch.long, device=device)\n",
    "output = decode(m.generate(context, max_new_tokens=len(sent), hard_seq = True)[0].tolist()) + \"]\"\n",
    "print(sent)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'Байден '[:32] #decode(data[0:0+32].tolist())\n",
    "context = torch.cat([\n",
    "#     PAREM_TOK,\n",
    "    torch.tensor(encode(sent), dtype=torch.long),\n",
    "    PAREM_TOK,\n",
    "    ], dim=0).reshape(1,-1).to(device)# torch.zeroes((1,1), dtype=torch.long, device=device)\n",
    "output = decode(m.generate(context, max_new_tokens=64, hard_seq=True)[0].tolist()) + \"]\"\n",
    "print(sent)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), model_finetuned_pt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(sent):\n",
    "    context = torch.cat([\n",
    "    #     REVERSE_TOK, \n",
    "        torch.tensor(encode(sent), dtype=torch.long),\n",
    "#         REVERSE_TOK, \n",
    "        ], dim=0).reshape(1,-1).to(device)# torch.zeroes((1,1), dtype=torch.long, device=device)\n",
    "    output = decode(m.generate(context, max_new_tokens=100, hard_seq = False)[0].tolist())\n",
    "    stop_pos = output.rfind('.')\n",
    "    if stop_pos > 2*len(sent):\n",
    "        output = output[:stop_pos]\n",
    "    return output\n",
    "\n",
    "test_cases = [\n",
    "    'как говорит радио Москвы',\n",
    "    'Был бы ум, а ПП=',\n",
    "    'Куда ни глянь везде ПФ='\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "    output = inference(sent)\n",
    "    print(\"Prompt : \", sent)\n",
    "    print(\"Predict: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
